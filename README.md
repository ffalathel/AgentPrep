# AgentPrep

**AgentPrep** is an intelligent machine learning preprocessing pipeline tool that combines AI agents with deterministic validation to automate data preparation tasks. It provides a guided, interactive experience for cleaning data, engineering features, and ensuring data quality and governance compliance.

## Features

- ğŸ¤– **AI-Powered Agents**: Leverages LLM agents (OpenAI, Anthropic, Gemini) for intelligent data quality improvements and feature engineering suggestions
- âœ… **Deterministic Validation**: All agent proposals are validated and executed deterministically for reproducibility
- ğŸ“Š **Interactive CLI**: User-friendly wizard interface - no configuration files required
- ğŸ¯ **Constraint Advisor**: Heuristic-based suggestions for pipeline constraints based on your dataset
- ğŸ”’ **Governance & Policy**: Built-in policy enforcement and data leakage detection
- ğŸ“¦ **Artifact Management**: Comprehensive artifact tracking, storage, and reporting
- ğŸ” **Metadata Tracking**: Full provenance and metadata generation for auditability

## Architecture

AgentPrep follows a multi-level pipeline architecture:

- **Level 0: Intent Validation** - Validates user configuration and constraints
- **Level 1: Data Ingestion & Schema Normalization** - Loads datasets, infers schemas, normalizes columns
- **Level 2: Data Quality Agent** - Profiles data quality and applies cleaning actions
- **Level 3: Metadata & Profiling Persistence** - Generates and stores comprehensive metadata
- **Level 4: Feature Engineering Agent** - Proposes and generates ML features
- **Level 5: Governance & Policy** - Enforces policies and detects data leakage
- **Level 6: Artifacts, Storage & Reporting** - Captures and exports pipeline artifacts

## Installation

### Prerequisites

- Python 3.8+
- pip

### Setup

1. Clone the repository:
```bash
git clone <repository-url>
cd AgentPrep
```

2. Install dependencies:
```bash
pip install pandas pydantic pyyaml
```

3. (Optional) Install LLM provider SDKs for AI agent features:
```bash
# For OpenAI
pip install openai

# For Anthropic
pip install anthropic

# For Google Gemini
pip install google-generativeai
```

## Quick Start

### Basic Usage

Run the interactive pipeline:

```bash
python cli.py run
```

The interactive wizard will guide you through:
1. **Dataset Selection**: Upload your CSV or Parquet file
2. **Task Configuration**: Select task type (classification, regression, time series, clustering)
3. **Target Column**: Choose your target variable from the dataset columns
4. **Model Family**: Select your intended model type (tree-based, linear, neural)
5. **Constraint Suggestions**: Get intelligent suggestions for pipeline constraints
6. **Output Path**: Specify where to save pipeline outputs

### Example Session

```bash
$ python cli.py run

============================================================
Welcome to AgentPrep!
============================================================

This interactive wizard will guide you through configuring your preprocessing pipeline.

Enter path to your dataset (CSV or Parquet): data/my_dataset.csv
âœ“ Dataset loaded: 10,000 rows, 15 columns

Available columns:
  1. age
  2. income
  3. education
  ...
  
Select target column (1-15): 3

Select task type:
  1. Classification
  2. Regression
  3. Time Series
  4. Clustering
Select (1-4) [1]: 1

...

âœ“ Intent validated successfully
Starting preprocessing pipeline...
âœ“ Pipeline completed successfully
```

### Command-Line Options

```bash
# Run with verbose logging
python cli.py run --verbose

# Specify output directory
python cli.py run --output ./results
```

## Configuration

### Environment Variables

Set API keys for LLM providers (optional - agents work without them in stub mode):

```bash
# OpenAI
export OPENAI_API_KEY="your-openai-api-key"

# Anthropic
export ANTHROPIC_API_KEY="your-anthropic-api-key"

# Google Gemini
export GEMINI_API_KEY="your-gemini-api-key"
```

**Note**: If no API keys are provided, agents will run in stub mode (no LLM proposals), but the pipeline will still execute deterministic operations.

### Intent Schema

The pipeline accepts configuration through an `IntentSchema` that includes:

- **Dataset**: Path to your dataset file
- **Task**: Task type and target column
- **Model**: Model family and interpretability requirements
- **Constraints**: Limits on features, interactions, and cardinality
- **Policies**: Outlier handling and other data policies

## Pipeline Levels

### Level 1: Data Ingestion & Schema Normalization

- Loads datasets from CSV or Parquet files
- Infers schema metadata (data types, nullability, distributions)
- Normalizes column names and data types
- Validates dataset against intent constraints

### Level 2: Data Quality Agent

- Profiles dataset quality (missing values, outliers, duplicates)
- LLM agent proposes data cleaning actions
- Deterministic executor validates and applies safe actions
- Tracks applied vs rejected actions

### Level 3: Metadata & Profiling Persistence

- Builds comprehensive pipeline metadata
- Records schema, quality profiles, and applied actions
- Writes metadata to disk for traceability

### Level 4: Feature Engineering Agent

- LLM agent proposes feature transformations
- Validates features for safety and compliance
- Generates features deterministically
- Tracks feature provenance

### Level 5: Governance & Policy

- Enforces policy rules (constraint violations, data leakage)
- Validates feature engineering proposals
- Detects potential data leakage issues
- Provides governance decisions

### Level 6: Artifacts, Storage & Reporting

- Captures all pipeline artifacts (datasets, schemas, features, metadata)
- Stores artifacts in organized directory structure
- Exports artifacts in multiple formats (JSON, CSV, Parquet, Markdown)
- Generates human-readable reports

## Project Structure

```
AgentPrep/
â”œâ”€â”€ cli/                    # CLI modules
â”‚   â”œâ”€â”€ interactive.py     # Interactive prompts
â”‚   â””â”€â”€ constraint_advisor.py  # Constraint suggestions
â”œâ”€â”€ core/                   # Core orchestration
â”‚   â””â”€â”€ orchestrator.py    # Pipeline orchestrator
â”œâ”€â”€ intent/                 # Intent validation
â”‚   â”œâ”€â”€ schema.py          # Intent schema definitions
â”‚   â””â”€â”€ validator.py       # Intent validation logic
â”œâ”€â”€ level1_ingestion/       # Data loading & normalization
â”œâ”€â”€ level2_quality/         # Data quality agent
â”œâ”€â”€ level3_metadata/       # Metadata generation
â”œâ”€â”€ level4_feature/         # Feature engineering agent
â”œâ”€â”€ level5_governance/      # Governance & policies
â”œâ”€â”€ level5_policy/          # Policy enforcement
â”œâ”€â”€ level6_artifacts/       # Artifact management
â”œâ”€â”€ utils/                  # Shared utilities
â”‚   â”œâ”€â”€ logging.py         # Logging setup
â”‚   â”œâ”€â”€ constants.py       # Application constants
â”‚   â”œâ”€â”€ file_helpers.py    # File utilities
â”‚   â””â”€â”€ llm_client.py      # LLM client wrapper
â””â”€â”€ cli.py                  # CLI entry point
```

## Supported Formats

- **Datasets**: CSV, Parquet
- **Configurations**: YAML, JSON (via interactive mode)
- **Output Formats**: JSON, CSV, Parquet, Markdown

## Exit Codes

- `0`: Success
- `1`: Invalid intent configuration
- `2`: Policy violation detected
- `3`: Runtime error

## Development

### Code Structure

- **Modular Design**: Each level is self-contained with clear interfaces
- **Type Safety**: Uses Pydantic for schema validation and type hints throughout
- **Logging**: Centralized logging via `utils.logging`
- **Error Handling**: Comprehensive error handling with custom exception types

## Contributing

1. Follow the existing code structure and naming conventions
2. Add tests for new features
3. Update documentation as needed
4. Ensure all tests pass before submitting

## License

[Add your license here]

## Support

For issues, questions, or contributions, please [open an issue](link-to-issues) or [create a pull request](link-to-prs).

---

**AgentPrep** - Intelligent ML Preprocessing with AI Agents
